# AI 集群使用规范（试行）
<br/>  

1. 集群管理员的工作时间为：周一~周五，8:30~17:30（法定节假日除外）。**<font color=red>工作时间以外，通过即时通讯软件联络的非紧急需求，原则上第二个工作日处理。</font>**  
<bc/>  

2. **<font color=red>训练管理任务最长运行时间 168 小时（即 7 天）</font>**。超过 168 小时，该任务将被后台自动删除，此为平台全局设置，无法提供单独延时服务。请用户在自己的任务里设置好断点、保存点，程序里设计好数据保存方式，即便环境被删除，也不影响已经计算出来的数据。  

3. **<font color=red>开发环境任务最长运行时间 72 小时（即 3 天）。</font>** 超过 72 小时，该任务将被后台自动删除，此为平台全局设置，无法提供单独延时服务。请用户在自己的任务里设置好断点、保存点，程序里设计好数据保存方式，即便环境被删除，也不影响已经计算出来的数据。请注意，开发环境的目的是调试程序，程序全部调通后提倡提交到训练环境里正式运行。  
   
4. **<font color=red>开发环境任务连续 4 小时 GPU 使用率低于 1% </font>** ，该任务会被平台识别为资源使用效率极差，将被后台自动删除，此为平台全局设置，无法提供单独延时服务。请用户在自己的任务里设置好断点、保存点，程序里设计好数据保存方式， 即便环境被删除，也不影响已经计算出来的数据。  

5. **<font color=red>每位用户限制最多启用 5 个开发环境。</font>** 此为平台全局设置，无法提供单独设置。  

6. **<font color=red>原则上每位用户限制最多调用 5 张 GPU、100 核 CPU。此设置可根据实际情况进行调整。</font>** 如确有需求，请发邮件至 hpc@cuhk.edu.cn ，同时抄送您的 PI（需两个地址同时发送），告知需求原因、需求数量、需求期限，我们将在评估后，为您调整资源限制。请注意，资源限制调整后，您需要受到额外的一些约束，例如：GPU 可不受限，但是不能调用某些资源组的资源。

7. **<font color=red>平台所有资源不是对每一位用户都全部可见</font>** ，如果某一款资源在官网上有公布，但是在您的账户下并不可见，而您又确实需要用到这款资源，请发邮件至 hpc@cuhk.edu.cn 咨询这款资源现在的使用状况是否还可以对您开放。

8. **<font color=red>每位用户最多可使用 800G 磁盘空间</font>** ，如有不够，首先各团队组内调节。例如：a、b 用户都属于 C 团队，a 的 800G 用完了，首先是看 b 是否有多余空间拿给 a用，b 也没有多余空间的时候，请发邮件至 hpc@cuhk.edu.cn，同时抄送您的 PI（需两个地址同时发送），提出申请，评估通过后才能申请 800G 以上磁盘空间，且每位用户最大扩容到 2T。共享服务器到集群的课题组，按照共享服务器的价值、共享策略，单另协商磁盘使用量。  

9. **<font color=red>每位用户最多可以保留 5 个镜像，用户自己制作的镜像，未经集群管理员批准，只可分享到组共享</font>** 。如需全校公开，请发邮件至 hpc@cuhk.edu.cn，申请镜像审核。

10. 如需提供包月服务，计费方式单另行告知。包月起始时间必须为每个月 1 日，结束时间可以自行确定。包月申请最晚请于 **<font color=red>下个月的前 7 个工作日提出</font>** 。例如：申请 2022/7/1~2022/7/20 这个时间段包月，则最晚要在 2022/6/22 邮件 hpc@cuhk.edu.cn，同时抄送您的 PI（需两个地址同时发送），提出申请。  

11. 服务器共享者对自己共享的计算节点有优先使用权。可以在需要的时候，**<font color=red> 提前 8 个自然日</font>** 申请对自己共享节点的独享。例如：对自己共享的节点申请 2022/7/1~2022/7/20 这个时间段独享 ， 则最晚要在 2022/6/23 邮件 hpc@cuhk.edu.cn，同时抄送您的 PI（需两个地址同时发送），提出申请。**<font color=red>但需注意，共享服务器的课题组需保证共享服务器的共享时长（以年度为单位，非自用CPU 核时不低于 20%），如共享时长过低，下个年度需接受共享策略的调整。</font>**  

12. 鼓励服务器共享，优惠条件根据大学的相关政策进行制定，并另行通知。 **<font color=red>共享服务器需接受平台的统一管理，且共享服务器需按照集群统一标准重装操作系统。</font>** 操作系统重装前，数据的备份、迁移等需用户自行完成。如已共享的服务器要退出集群共享，需提前一个月联系集群管理员，由集群管理员引导完成退出流程。

13. **<font color=red>账号注销时该账号下的所有数据将被删除</font>**，如需保留数据，用户需在账号注销前自行将数据迁出。

14. 包括但不限于因毕业、离职等原因，不再使用集群的用户应主动联系集群管理员注销账号。**<font color=red>如集群管理员未接到任何联系，超过 400 天未登陆的账号将被认为不再使用集群，该账号以及该账号下的所有数据（包括在共享目录下属于该账号的文件），集群管理员将不做通知，直接删除。</font>**  

15. 该规则将于 2023 年 01 月 01 日起实施。  

# AI 集群注意事项

1. AI 集群都是 GPU 服务器，主要满足 GPU 的计算需求，如大量使用 CPU，不用或者用很少的 GPU，会造成 GPU 资源的浪费，集群管理员将对这类用户采取如下措施：  
① 如果集群管理员后台监测到某用户长期采用浪费 GPU 资源的用法，即便没有触发费用折算条件，也将限定一些计算节点对这类用户不可见。  
② 如果使用了一台计算节点≥1/2 的 CPU，使用的 GPU 却≤1/3（不够整除的，一律按照整数部分加 1 计），则需在已产生的费用的基础上，再加上折算的 GPU费用。折算举例如下：  
   - 例一：warshel-gpu01 节点有 32 核 CPU、8 张 GPU。如果使用了 16 核 CPU，2 张 GPU，触发费用折算条件，将在 16 核 CPU、2 张 GPU 费用的基础上，再加上 2 张 GPU 的费用。  
   - 例二：kobilka-gpu01 节点有 32 核 CPU、2 张 GPU。如果使用了 17 核 CPU，1 张 GPU，触发费用折算条件，将在 17 核 CPU、1 张 GPU 费用的基础上，再加上 1 张 GPU 的费用。  
   - 例三：sribd-gpu01 节点有 128 核 CPU、8 张 GPU。如果使用了 63 核 CPU，0 张 GPU，没有触发费用折算条件，正常计费，但长期如此使用，集群管理员会调整资源策略，让 sribd-gpu01 节点对该用户不可见。  

2. 集群里 2080Ti、3080Ti、3090、RTX 所属的计算节点，显卡的 GPU 驱动版本是 495.44；4090 所属的计算节点，显卡的 GPU 驱动版本是 525.105.17；这以外的计算节点，显卡的 GPU 驱动版本是 470.82.01。因此提交到驱动版本是 495.44 的训练任务，在开发调试的时候，最好也用驱动版本是 495.44 的节点来调试。  
   
3. 集群默认对提交的任务不做内存限制，由计算节点 OS 自行调度内存使用量。但内存耗尽会触发系统自我保护机制，终结触发内存使用量阈值的任务进程。因此如果已知自己的任务比较耗费内存，建议找比较空闲的节点投递任务。现阶段，不建议通过任何方法强行指定任务所分配的内存大小。目前计算节点有限，这种指定可能会造成一个节点只跑一个任务。  

4. 使用节点缓存会大幅提升计算效率，但是否使用计算节点缓存，请用户合理做出衡量。如果数据集的文件数很多（比如有 100w 张图片），则节点缓存会非常耗时。  

5. 每个组只能分配一个组管理员角色，组管理员有一定的管理权限，能查看和调节组内成员的资源状况，具体使用方法见操作手册。每个组需要哪位用户是组管理员，请发邮件至邮件 hpc@cuhk.edu.cn，同时抄送您的 PI（需两个地址同时发送），提出申请。  
