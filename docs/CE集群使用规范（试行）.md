# CE集群使用规范（试行）  

1. 集群管理员的工作时间为：周一~周五，8:30~17:30（法定节假日除外）。**<font color=red>工作时间以外，通过即时通讯软件联络的非紧急需求，原则上第二个工作日处理。</font>**  
 
2. **<font color=red>任务最长运行时间168小时（即7天）</font>**。超过168小时，该任务将被后台自动删除，此为平台全局设置，无法提供单独延时服务。请用户在自己的任务里设置好断点、保存点，程序里设计好数据保存方式，即便环境被删除，也不影响已经计算出来的数据。  
   
3. 在CE集群没有缓存功能前，**<font color=red>禁止用户将个人数据传至任何计算节点。一经发现，该数据将不做通知，强制清除。</font>** 如果将来有了缓存功能，届时使用方法和数据清除策略再行通知。

4. **<font color=red>原则上每位用户限制最多调用5张GPU、100核CPU。此设置可根据实际情况进行调整。</font>** 如确有需求，请发邮件至 hpc@cuhk.edu.cn ，同时抄送您的PI（需两个地址同时发送），告知需求原因、需求数量、需求期限，我们将在评估后，为您调整资源限制。请注意，资源限制调整后，您需要受到额外的一些约束，例如：放开对GPU和CPU的限制，但是不能使用某些分区。
   
5. **<font color=red>每位用户最多可使用800G磁盘空间</font>**，如有不够，首先各团队组内调节。例如：a、b用户都属于C团队，a的800G用完了，首先是看b是否有多余空间拿给a用，b也没有多余空间的时候，请发邮件至 hpc@cuhk.edu.cn ，同时抄送您的PI（需两个地址同时发送），提出申请，评估通过后才能申请800G以上磁盘空间，且每位用户最大扩容到2T。共享服务器到集群的课题组，按照共享服务器的价值、共享策略，单另协商磁盘使用量。  
   
6. 如需提供包月服务，计费方式单另行告知。 包月起始时间必须为每个月1日，结束时间可以自行确定。 包月申请最晚请于 **<font color=red>下个月的前7个工作日提出。</font>** 例如：申请2022/7/1~2022/7/20这个时间段包月，则最晚要在2022/6/22邮件 hpc@cuhk.edu.cn ，同时抄送您的PI（需两个地址同时发送），提出申请。  
   
7. 服务器共享者对自己共享的计算节点有优先使用权。可以在需要的时候， **<font color=red>提前8个自然日</font>** 申请对自己共享节点的独享。例如：对自己共享的节点申请2022/7/1~2022/7/20这个时间段独享，则最晚要在2022/6/23邮件 hpc@cuhk.edu.cn ，同时抄送您的PI（需两个地址同时发送），提出申请。**<font color=red>但需注意，共享服务器的课题组需保证共享服务器的共享时长（以年度为单位，非自用CPU核时不低于20%），如共享时长过低，下个年度需接受共享策略的调整。</font>**  
   
8. 鼓励服务器共享，优惠条件根据大学的相关政策进行制定，并另行通知。**<font color=red>共享服务器需接受平台的统一管理，且共享服务器需按照集群统一标准重装操作系统。</font>** 操作系统重装前，数据的备份、迁移等需用户自行完成。如已共享的服务器要退出集群共享，需提前一个月联系集群管理员，由集群管理员引导完成退出流程。  
   
9. **<font color=red>账号注销时该账号下的所有数据将被删除</font>** ，如需保留数据，用户需在账号注销前自行将数据迁出。  
10. 包括但不限于因毕业、离职等原因，不再使用集群的用户应主动联系集群管理员注销账号。**<font color=red>如集群管理员未接到任何联系，超过400天未登陆的账号将被认为不再使用集群，该账号以及该账号下的所有数据（包括在共享目录下属于该账号的文件），集群管理员将不做通知，直接删除。</font>**  
11. 该规则将于2023年01月01日起实施。

<br/><br/>

# <center>CE集群注意事项<center>  
1. CE集群既有纯CPU计算节点，也有GPU计算节点，如在GPU计算节点大量使用CPU，不用或者用很少的GPU，会造成GPU资源的浪费，集群管理员将对这类用户采取如下措施：  
① 如果集群管理员后台监测到某用户长期采用浪费GPU资源的用法，即便没有触发费用折算条件，也将限定一些计算节点对这类用户不可见。  
② 如果使用了一台计算节点≥1/2的CPU，使用的GPU却≤1/3（不够整除的，一律按照整数部分加1计），则需在已产生的费用的基础上，再加上折算的GPU费用。折算举例如下：  
- 例一：warshel-gpu01节点有32核CPU、8张GPU。如果使用了16核CPU，2张GPU，触发费用折算条件，将在16核CPU、2张GPU费用的基础上，再加上2张GPU的费用。  
- 例二：kobilka-gpu01节点有32核CPU、2张GPU。如果使用了17核CPU，1张GPU，触发费用折算条件，将在17核CPU、1张GPU费用的基础上，再加上1张GPU的费用。  
- 例三：sribd-gpu01节点有128核CPU、8张GPU。如果使用了63核CPU，0张GPU，没有触发费用折算条件，正常计费，但长期如此使用，集群管理员会调整资源策略，让sribd-gpu01节点对该用户不可见。

2. 集群默认对提交的任务不做内存限制，由计算节点OS自行调度内存使用量。但内存耗尽会触发系统自我保护机制，终结触发内存使用量阈值的任务进程。因此如果已知自己的任务比较耗费内存，建议找比较空闲的节点投递任务。现阶段，不建议通过任何方法强行指定任务所分配的内存大小。目前计算节点有限，这种指定可能会造成一个节点只跑一个任务。  
3. 每个组只能分配一个组管理员角色，组管理员有一定的管理权限，具体使用方法见操作手册。每个组需要哪位用户是组管理员，请发邮件至邮件 hpc@cuhk.edu.cn ，同时抄送您的PI（需两个地址同时发送），提出申请。